#!/usr/bin/env python3
"""
Script principal pour exÃ©cuter le pipeline complet de scraping et d'analyse
"""

import sys
import os
import argparse
from datetime import datetime

# Ajout du chemin vers src
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from scraper import AgroDataScraper
from data_processor import AgroDataProcessor
from visualizations import AgroDataVisualizer

def run_full_pipeline():
    """ExÃ©cute le pipeline complet de scraping Ã  visualisation"""
    print("ğŸš€ DÃ©marrage du pipeline complet de scraping agroalimentaire")
    print("=" * 60)
    
    # Ã‰tape 1: Scraping
    print("\nğŸ“¡ Ã‰tape 1: Scraping des donnÃ©es")
    print("-" * 30)
    
    try:
        scraper = AgroDataScraper()
        
        categories = {
            'LÃ©gumes': 'https://rnm.franceagrimer.fr/prix?LEGUMES',
            'Fruits': 'https://rnm.franceagrimer.fr/prix?FRUITS',
            'Viande': 'https://rnm.franceagrimer.fr/prix?VIANDE',
            'Beurre_Oeuf_Fromage': 'https://rnm.franceagrimer.fr/prix?BEURRE-OEUF-FROMAGE'
        }
        
        all_results = []
        
        for category_name, category_url in categories.items():
            print(f"  ğŸ“‚ Scraping de la catÃ©gorie: {category_name}")
            category_data = scraper.scrape_category(category_name, category_url)
            all_results.extend(category_data)
            
            # Sauvegarde intermÃ©diaire
            if category_data:
                scraper.save_data(category_data, f'data/{category_name.lower()}_prices.csv')
        
        # Sauvegarde finale
        if all_results:
            scraper.save_data(all_results, 'data/all_agro_prices.csv')
            print(f"  âœ… Scraping terminÃ©: {len(all_results)} enregistrements collectÃ©s")
        else:
            print("  âŒ Aucune donnÃ©e collectÃ©e")
            return False
            
    except Exception as e:
        print(f"  âŒ Erreur lors du scraping: {e}")
        return False
    
    # Ã‰tape 2: Traitement des donnÃ©es
    print("\nğŸ§¹ Ã‰tape 2: Traitement et nettoyage des donnÃ©es")
    print("-" * 30)
    
    try:
        processor = AgroDataProcessor()
        
        # Chargement et nettoyage
        df = processor.load_data('data/all_agro_prices.csv')
        if df.empty:
            print("  âŒ Aucune donnÃ©e Ã  traiter")
            return False
        
        clean_df = processor.clean_data(df)
        enriched_df = processor.add_derived_features(clean_df)
        
        # Sauvegarde
        processor.save_processed_data(enriched_df, 'data/processed_agro_prices.csv')
        
        # Statistiques
        stats = processor.generate_summary_stats(enriched_df)
        print("  ğŸ“Š Statistiques des donnÃ©es traitÃ©es:")
        for key, value in stats.items():
            print(f"    - {key}: {value}")
        
        print("  âœ… Traitement terminÃ© avec succÃ¨s")
        
    except Exception as e:
        print(f"  âŒ Erreur lors du traitement: {e}")
        return False
    
    # Ã‰tape 3: Visualisations
    print("\nğŸ“ˆ Ã‰tape 3: GÃ©nÃ©ration des visualisations")
    print("-" * 30)
    
    try:
        visualizer = AgroDataVisualizer()
        plots = visualizer.generate_all_plots()
        
        successful_plots = sum(1 for plot in plots.values() if plot is not None)
        print(f"  âœ… {successful_plots}/{len(plots)} graphiques gÃ©nÃ©rÃ©s avec succÃ¨s")
        
        print("  ğŸ“ Graphiques disponibles dans 'static/plots/'")
        for plot_name, plot in plots.items():
            status = "âœ…" if plot else "âŒ"
            print(f"    {status} {plot_name}")
        
    except Exception as e:
        print(f"  âŒ Erreur lors de la gÃ©nÃ©ration des graphiques: {e}")
        return False
    
    # Ã‰tape 4: Lancement du dashboard
    print("\nğŸŒ Ã‰tape 4: Lancement du dashboard")
    print("-" * 30)
    
    try:
        import subprocess
        import webbrowser
        import time
        
        print("  ğŸš€ Lancement de Streamlit...")
        
        # Lancement de Streamlit en arriÃ¨re-plan
        streamlit_process = subprocess.Popen([
            sys.executable, "-m", "streamlit", "run", "app.py", "--server.headless", "true"
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        # Attendre que le serveur dÃ©marre
        time.sleep(5)
        
        print("  ğŸŒ Dashboard disponible Ã : http://localhost:8501")
        print("  ğŸ“ Appuyez sur Ctrl+C pour arrÃªter le serveur")
        
        # Ouvrir le navigateur (optionnel)
        try:
            webbrowser.open('http://localhost:8501')
        except:
            print("    (Impossible d'ouvrir automatiquement le navigateur)")
        
        # Attendre que l'utilisateur arrÃªte
        try:
            streamlit_process.wait()
        except KeyboardInterrupt:
            print("\n  ğŸ›‘ ArrÃªt du serveur Streamlit...")
            streamlit_process.terminate()
        
    except Exception as e:
        print(f"  âŒ Erreur lors du lancement du dashboard: {e}")
        return False
    
    print("\nğŸ‰ Pipeline terminÃ© avec succÃ¨s!")
    return True

def run_scraping_only():
    """ExÃ©cute uniquement le scraping"""
    print("ğŸ“¡ Lancement du scraping uniquement")
    
    try:
        scraper = AgroDataScraper()
        
        categories = {
            'LÃ©gumes': 'https://rnm.franceagrimer.fr/prix?LEGUMES',
            'Fruits': 'https://rnm.franceagrimer.fr/prix?FRUITS',
            'Viande': 'https://rnm.franceagrimer.fr/prix?VIANDE',
            'Beurre_Oeuf_Fromage': 'https://rnm.franceagrimer.fr/prix?BEURRE-OEUF-FROMAGE'
        }
        
        all_results = []
        
        for category_name, category_url in categories.items():
            print(f"Scraping de: {category_name}")
            category_data = scraper.scrape_category(category_name, category_url)
            all_results.extend(category_data)
        
        if all_results:
            scraper.save_data(all_results, 'data/all_agro_prices.csv')
            print(f"âœ… {len(all_results)} enregistrements collectÃ©s")
        else:
            print("âŒ Aucune donnÃ©e collectÃ©e")
            
    except Exception as e:
        print(f"âŒ Erreur: {e}")

def run_processing_only():
    """ExÃ©cute uniquement le traitement des donnÃ©es"""
    print("ğŸ§¹ Lancement du traitement des donnÃ©es")
    
    try:
        processor = AgroDataProcessor()
        
        df = processor.load_data('data/all_agro_prices.csv')
        if df.empty:
            print("âŒ Aucune donnÃ©e Ã  traiter")
            return
        
        clean_df = processor.clean_data(df)
        enriched_df = processor.add_derived_features(clean_df)
        
        processor.save_processed_data(enriched_df, 'data/processed_agro_prices.csv')
        
        stats = processor.generate_summary_stats(enriched_df)
        print("ğŸ“Š Statistiques:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        print("âœ… Traitement terminÃ©")
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")

def run_visualizations_only():
    """ExÃ©cute uniquement la gÃ©nÃ©ration des visualisations"""
    print("ğŸ“ˆ GÃ©nÃ©ration des visualisations")
    
    try:
        visualizer = AgroDataVisualizer()
        plots = visualizer.generate_all_plots()
        
        successful_plots = sum(1 for plot in plots.values() if plot is not None)
        print(f"âœ… {successful_plots}/{len(plots)} graphiques gÃ©nÃ©rÃ©s")
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")

def main():
    """Fonction principale avec gestion des arguments"""
    parser = argparse.ArgumentParser(description='Pipeline de scraping et analyse agroalimentaire')
    
    parser.add_argument(
        '--mode', 
        choices=['full', 'scraping', 'processing', 'visualizations', 'dashboard'],
        default='full',
        help='Mode d\'exÃ©cution (default: full)'
    )
    
    args = parser.parse_args()
    
    print("ğŸ¥¬ Dashboard Agroalimentaire - Pipeline de Scraping")
    print(f"ğŸ• DÃ©marrÃ© le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # VÃ©rification des dossiers nÃ©cessaires
    os.makedirs('data', exist_ok=True)
    os.makedirs('static/plots', exist_ok=True)
    
    # ExÃ©cution selon le mode
    if args.mode == 'full':
        success = run_full_pipeline()
    elif args.mode == 'scraping':
        run_scraping_only()
        success = True
    elif args.mode == 'processing':
        run_processing_only()
        success = True
    elif args.mode == 'visualizations':
        run_visualizations_only()
        success = True
    elif args.mode == 'dashboard':
        # Lancement du dashboard uniquement
        os.system("streamlit run app.py")
        success = True
    else:
        print("âŒ Mode non reconnu")
        success = False
    
    if success:
        print("\nğŸ‰ OpÃ©ration terminÃ©e avec succÃ¨s!")
    else:
        print("\nâŒ OpÃ©ration Ã©chouÃ©e")
        sys.exit(1)

if __name__ == "__main__":
    main()
